{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXaDmsMjyqDrQHEPl0kYQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourabh92133/pricer_llm_fine_tuning/blob/main/notebooks/inference_and_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lX7gCp313mf9"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
        "from datasets import Dataset, load_dataset,DatasetDict\n",
        "from datetime import datetime\n",
        "from peft import PeftModel\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "Umsq7vpn728v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "Project_Name=\"pricer\"\n",
        "HF_USER=\"sourabh004\"         # you use you HF-USER name\n",
        "\n",
        "# the run\n",
        "Run_Name=\"25-12-28_07.13.56\"\n",
        "Project_Run_Name=f\"{Project_Name}-{Run_Name}\"      # or directly use this \"pricer-25-12-28_07.13.56\"\n",
        "Fine_Tuned_Model_Name=f\"{HF_USER}/{Project_Run_Name}\"    # or directly use this from hugging face \"sourabh004/pricer-25-12-28_07.13.56\"\n",
        "# Revision=\"d7e54ea4678868601675cb7072a79deb5c8a2786\"     # checkpoint that you want to run\n",
        "Revision=None      # if you want to run latest checkpoint\n",
        "\n",
        "# dataset\n",
        "DATASET_NAME = \"ed-donner/pricer-data\"      # i used this data but you can use your own dataset\n",
        "MAX_SEQUENCE_LENGTH=182\n",
        "\n",
        "# hyperparameter for Qlora\n",
        "Quant_4bit=True\n",
        "%matplotlib inline     # this is only for notebook\n",
        "\n",
        "# Used for writing to output in color\n",
        "\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
      ],
      "metadata": {
        "id": "8hUxa7GR9Hmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hf_token=userdata.get('HF_TOKEN')\n",
        "# login(hf_token,add_to_git_credential=True)\n",
        "# Hugging Face Authentication\n",
        "\n",
        "# If running on Google Colab, you may set HF_TOKEN like this:\n",
        "\n",
        "# os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if HF_TOKEN is None:\n",
        "    raise ValueError(\n",
        "        \"HF_TOKEN environment variable not set. \"\n",
        "        \"Please set it via environment variables or Colab userdata.\"\n",
        "    )\n",
        "\n",
        "login(HF_TOKEN, add_to_git_credential=True)\n"
      ],
      "metadata": {
        "id": "kdwePkwb_tSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(DATASET_NAME)\n",
        "dataset\n",
        "test=dataset[\"test\"]\n",
        "train=dataset[\"train\"]"
      ],
      "metadata": {
        "id": "Qunh3IvVAPIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Quant_4bit:\n",
        "  quant_config=BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "      bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config=BitsAndBytesConfig(\n",
        "      load_in_8bit=True,\n",
        "      bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ],
      "metadata": {
        "id": "6PgCGfWKArbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading Tokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained(BASE_MODEL,trust_remote_code=True)\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "tokenizer.pad_side=\"right\"\n",
        "# loading model\n",
        "base_model=AutoModelForCausalLM.from_pretrained(BASE_MODEL,quantization_config=quant_config,device_map=\"auto\")\n",
        "base_model.generation_config.pad_token_id=tokenizer.pad_token_id\n",
        "\n",
        "# load fine tuned model\n",
        "if Revision:\n",
        "  fine_tuned_model=PeftModel.from_pretrained(base_model,Fine_Tuned_Model_Name,revision=Revision)\n",
        "else:\n",
        "  fine_tuned_model=PeftModel.from_pretrained(base_model,Fine_Tuned_Model_Name)\n",
        "\n",
        "\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "Zbk0Ep6pBMph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4A5sEnLGC8Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_price(s):\n",
        "    if \"Price is $\" in s:\n",
        "      contents = s.split(\"Price is $\")[1]\n",
        "      contents = contents.replace(',','')\n",
        "      match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)      # this part will search for digits\n",
        "      return float(match.group()) if match else 0\n",
        "    return 0"
      ],
      "metadata": {
        "id": "SOQQfBNQDFHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_price(\"Price is $a fabulous 899.99$ or so\")"
      ],
      "metadata": {
        "id": "89Vn6uabDWeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original prediction function takes the most likely next token\n",
        "def model_predict(prompt):\n",
        "  inputs=tokenizer.encode(prompt,return_tensors=\"pt\").to(\"cuda\")    # this will return pytorch tensor then moves data to gpu\n",
        "  attention_mask=torch.ones(inputs.shape,device=\"cuda\")     # to remove warnings\n",
        "  outputs=fine_tuned_model.generate(inputs,attention_mask=attention_mask,max_new_tokens=3,num_return_sequences=1)\n",
        "  output=tokenizer.decode(outputs[0])\n",
        "  return extract_price(output)"
      ],
      "metadata": {
        "id": "neDCSEEYDdSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An improved prediction function takes a weighted average of the top 3 choices\n",
        "# This code would be more complex if we couldn't take advantage of the fact\n",
        "# That Llama generates 1 token for any 3 digit number\n",
        "\n",
        "top_K = 3\n",
        "\n",
        "def improved_model_predict(prompt, device=\"cuda\"):\n",
        "    set_seed(42)\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones(inputs.shape, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = fine_tuned_model(inputs, attention_mask=attention_mask)\n",
        "        next_token_logits = outputs.logits[:, -1, :].to('cpu')\n",
        "\n",
        "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "    top_prob, top_token_id = next_token_probs.topk(top_K)\n",
        "    prices, weights = [], []\n",
        "    for i in range(top_K):\n",
        "      predicted_token = tokenizer.decode(top_token_id[0][i])\n",
        "      probability = top_prob[0][i]\n",
        "      try:\n",
        "        result = float(predicted_token)\n",
        "      except ValueError as e:\n",
        "        result = 0.0\n",
        "      if result > 0:\n",
        "        prices.append(result)\n",
        "        weights.append(probability)\n",
        "    if not prices:\n",
        "      return 0.0, 0.0\n",
        "    total = sum(weights)\n",
        "    weighted_prices = [price * weight / total for price, weight in zip(prices, weights)]\n",
        "    return sum(weighted_prices).item()"
      ],
      "metadata": {
        "id": "OAd58gBnDdlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tester class-> you shpuld understand this\n",
        "class Tester:\n",
        "\n",
        "    def __init__(self, predictor, data, title=None, size=250):\n",
        "        self.predictor = predictor\n",
        "        self.data = data\n",
        "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
        "        self.size = size\n",
        "        self.guesses = []\n",
        "        self.truths = []\n",
        "        self.errors = []\n",
        "        self.sles = []\n",
        "        self.colors = []\n",
        "\n",
        "    def color_for(self, error, truth):\n",
        "        if error<40 or error/truth < 0.2:\n",
        "            return \"green\"\n",
        "        elif error<80 or error/truth < 0.4:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "\n",
        "    def run_datapoint(self, i):\n",
        "        datapoint = self.data[i]\n",
        "        guess = self.predictor(datapoint[\"text\"])\n",
        "        truth = datapoint[\"price\"]\n",
        "        error = abs(guess - truth)\n",
        "        log_error = math.log(truth+1) - math.log(guess+1)\n",
        "        sle = log_error ** 2\n",
        "        color = self.color_for(error, truth)\n",
        "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
        "        self.guesses.append(guess)\n",
        "        self.truths.append(truth)\n",
        "        self.errors.append(error)\n",
        "        self.sles.append(sle)\n",
        "        self.colors.append(color)\n",
        "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
        "\n",
        "    def chart(self, title):\n",
        "        max_error = max(self.errors)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        max_val = max(max(self.truths), max(self.guesses))\n",
        "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
        "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Model Estimate')\n",
        "        plt.xlim(0, max_val)\n",
        "        plt.ylim(0, max_val)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    def report(self):\n",
        "        average_error = sum(self.errors) / self.size\n",
        "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
        "        hits = sum(1 for color in self.colors if color==\"green\")\n",
        "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
        "        self.chart(title)\n",
        "\n",
        "    def run(self):\n",
        "        self.error = 0\n",
        "        for i in range(self.size):\n",
        "            self.run_datapoint(i)\n",
        "        self.report()\n",
        "\n",
        "    @classmethod\n",
        "    def test(cls, function, data):\n",
        "        cls(function, data).run()"
      ],
      "metadata": {
        "id": "j6DrHFBsDiA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(model_predict, test)"
      ],
      "metadata": {
        "id": "xdZuar31Fz41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(improved_model_predict, test)"
      ],
      "metadata": {
        "id": "wUnkmWE9D27k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ro5eKugEhbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}