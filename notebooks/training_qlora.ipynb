{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLWHv2jel8KN/gONd5CgNt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourabh92133/pricer_llm_fine_tuning/blob/main/notebooks/training_qlora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLoRA Fine-Tuning â€“ Training Notebook\n",
        "\n",
        "This notebook performs QLoRA fine-tuning of LLaMA 3.1 (8B) on the\n",
        "`ed-donner/pricer-data` dataset for price prediction.\n",
        "\n",
        "Responsibilities:\n",
        "- Load dataset\n",
        "- Configure 4-bit QLoRA\n",
        "- Fine-tune model using TRL SFTTrainer\n",
        "- Push LoRA adapters to Hugging Face Hub\n"
      ],
      "metadata": {
        "id": "1Uq2_YaGoA8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onVaZUWKJFNQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "H0gjZlI4N9Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Base_Model=\"meta-llama/Meta-Llama-3.1-8B\"\n",
        "Project_Name=\"pricer\"\n",
        "HF_USER=\"sourabh004\"     # you use you HF-USER name\n",
        "Dataset_Name=\"ed-donner/pricer-data\"    # i used this data but you can use your own dataset\n",
        "MAX_SEQUENCE_LENGTH=182\n",
        "\n",
        "# Run name for saving the model in the hub\n",
        "\n",
        "Run_Name=f\"{datetime.now():%y-%m-%d_%H.%M.%S}\"     # this will show you current time\n",
        "Project_Run_Name=f\"{Project_Name}-{Run_Name}\"      # this will show you current project name\n",
        "Hf_Model_Name=f\"{HF_USER}/{Project_Run_Name}\"      # this is hugging face project name\n",
        "\n",
        "# Hyperparameter for Qlora\n",
        "Lora_R=32\n",
        "Lora_Alpha=64\n",
        "Lora_Target_Modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        "Lora_Dropout=0.1     # 10 percent of total neurons will be deactivated everytime to prevent form overfitting\n",
        "Quant_4_Bit=True\n",
        "\n",
        "# Hyperparamter for Training\n",
        "Epochs=1\n",
        "Batch_Size=1\n",
        "Gradient_Accumulation=1\n",
        "Learning_Rate=1e-4\n",
        "Learning_Scheduler_Type=\"cosine\"\n",
        "Warmup_Ratio=0.3     # so learning rate will first warm up then it will decrease\n",
        "Optimizer=\"paged_adamw_32bit\"\n",
        "\n",
        "# now saving model to hub\n",
        "Steps=50\n",
        "Save_Steps=2000\n",
        "LOG_TO_WANDB=True\n",
        "\n",
        "%matplotlib inline      # this  is only for notebook"
      ],
      "metadata": {
        "id": "-8KZI1nOOELR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Hf_Model_Name"
      ],
      "metadata": {
        "id": "aaTFzCP8S5sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Authentication\n",
        "\n",
        "# If running on Google Colab, you may set HF_TOKEN like this:\n",
        "\n",
        "# os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if HF_TOKEN is None:\n",
        "    raise ValueError(\n",
        "        \"HF_TOKEN environment variable not set. \"\n",
        "        \"Please set it via environment variables or Colab userdata.\"\n",
        "    )\n",
        "\n",
        "login(HF_TOKEN, add_to_git_credential=True)\n"
      ],
      "metadata": {
        "id": "mNo-Qj8TUmo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# login to weights and biases\n",
        "# This notebook expects WANDB_API_KEY to be set as an environment variable.\n",
        "# Example (Google Colab):\n",
        "\n",
        "# os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB\")\n",
        "\n",
        "if LOG_TO_WANDB:\n",
        "    wandb.login()\n",
        "\n",
        "    # All logs from this run will be grouped under this project name\n",
        "    os.environ[\"WANDB_PROJECT\"] = Project_Name\n",
        "\n",
        "    # Upload model weights at each checkpoint if logging is enabled\n",
        "    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "\n",
        "    # Track gradients during training\n",
        "    os.environ[\"WANDB_WATCH\"] = \"gradients\"\n"
      ],
      "metadata": {
        "id": "cL7NW5SxVDW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_dataset(Dataset_Name)"
      ],
      "metadata": {
        "id": "geUz3avYZg8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=dataset[\"train\"]\n",
        "test=dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "GEl3uYgWZ25T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us reduce the size of training data to 20k datapoints\n",
        "train=train.select(range(20000))      # i reduced the size of training datset , it's your choice whether you want to reduce or not"
      ],
      "metadata": {
        "id": "7-aoGR4Cai3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=Project_Name,name=Run_Name)   # this is to start new experiment run"
      ],
      "metadata": {
        "id": "nJB6q74-bqjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Quant_4_Bit:\n",
        "  quant_config=BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "      bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config=BitsAndBytesConfig(\n",
        "      load_in_8bit=True,\n",
        "      bnb_8bit_compute_dtype=torch.bloat16,\n",
        "  )"
      ],
      "metadata": {
        "id": "HuAhqvJEcOdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer and model\n",
        "tokenizer=AutoTokenizer.from_pretrained(Base_Model,trust_remote_code=True)\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "tokenizer.padding_side=\"right\"\n",
        "\n",
        "base_model=AutoModelForCausalLM.from_pretrained(\n",
        "    Base_Model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "XBTjjszlj7l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us build data collator that will tell trainer everything written till price is $ is context and it has to predict token next to it\n",
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "response_template=\"Price is $\"                 # this will tell trainer that to teach model to predict token next to this\n",
        "collator=DataCollatorForCompletionOnlyLM(response_template,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "8fRGusqUlist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration\n",
        "\n",
        "In this section, we set up the configuration required for fine-tuning.\n",
        "\n",
        "We create **two key objects**:\n",
        "\n",
        "1. **LoRA Configuration (`LoraConfig`)**  \n",
        "   Defines the Low-Rank Adaptation (LoRA) hyperparameters used for parameter-efficient fine-tuning, such as rank, scaling factor, dropout, and target modules.\n",
        "\n",
        "2. **Training Configuration (`SFTConfig`)**  \n",
        "   Specifies the overall training setup, including batch size, learning rate, optimizer, scheduler, logging, checkpointing, and Hugging Face Hub integration.\n"
      ],
      "metadata": {
        "id": "zIolq8CPnD_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_parameters=LoraConfig(\n",
        "    r=Lora_R,\n",
        "    lora_alpha=Lora_Alpha,\n",
        "    lora_dropout=Lora_Dropout,\n",
        "    target_modules=Lora_Target_Modules,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "train_parameters=SFTConfig(\n",
        "    output_dir=Project_Run_Name,\n",
        "    num_train_epochs=Epochs,\n",
        "    per_device_train_batch_size=Batch_Size,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=Gradient_Accumulation,\n",
        "    optim=Optimizer,\n",
        "    save_steps=Save_Steps,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=Steps,\n",
        "    learning_rate=Learning_Rate,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=Warmup_Ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=Learning_Scheduler_Type,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=Run_Name,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=Hf_Model_Name,\n",
        "    hub_private_repo=True\n",
        ")\n",
        "fine_tuning=SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters,\n",
        "    data_collator=collator\n",
        ")"
      ],
      "metadata": {
        "id": "dPU-5NB-nH6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bias = null ensures that bias i.e model parameters are not changed only lora parameters are changed or trained\n",
        "\n",
        "# For \"CAUSAL_LM\":\n",
        "# Adapters are added to:\n",
        "\n",
        "  # attention projections (q_proj, v_proj, etc.)\n",
        "\n",
        "  # Training uses next-token prediction\n",
        "\n",
        "# If you give wrong task_type:\n",
        "\n",
        "  # LoRA may attach incorrectly\n",
        "\n",
        "  # Training can silently degrade"
      ],
      "metadata": {
        "id": "UKDKvQHzpYKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will kick off fine tunning\n",
        "fine_tuning.train()\n",
        "# pushing fine tuned model to hugging face hub\n",
        "fine_tuning.push_to_hub(Hf_Model_Name,private=True)\n",
        "print(f\"save to hub :{Project_Run_Name}\")"
      ],
      "metadata": {
        "id": "g_cvx7xKzKjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "7svwIRN51Uwr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}